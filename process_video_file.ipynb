{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1PRGTX2Gg6v6eWm5hkxNwQa68sCBt64a0",
      "authorship_tag": "ABX9TyNiwGpfRs6hN//fVdUXTxoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jasonhcwong/face-recognition-system/blob/main/process_video_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the repo and install required library"
      ],
      "metadata": {
        "id": "TxZTVYHNm4ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jasonhcwong/face-recognition-system.git\n",
        "%cd face-recognition-system\n",
        "\n",
        "!pip install mtcnn"
      ],
      "metadata": {
        "id": "XIXmys80kMAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required library"
      ],
      "metadata": {
        "id": "CuBKP_9unL8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bdoAXSerh9RD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import face_preprocess\n",
        "\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "YmPtYlcdnRV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidian_distance(embeddings1, embeddings2):\n",
        "    embeddings1 = embeddings1/np.linalg.norm(embeddings1, axis=0, keepdims=True)\n",
        "    embeddings2 = embeddings2/np.linalg.norm(embeddings2, axis=0, keepdims=True)\n",
        "    dist = np.sqrt(np.sum(np.square(np.subtract(embeddings1, embeddings2))))\n",
        "    return dist\n",
        "\n",
        "def calc_confidence(distance, threshold=1.2333):\n",
        "    return 0 if distance >= threshold else (1.0-distance/threshold)\n",
        "\n",
        "def compare_faces(embeddings1, embeddings2):\n",
        "    dist = euclidian_distance(embeddings1, embeddings2)\n",
        "    conf = calc_confidence(dist)\n",
        "    return dist, conf\n",
        "\n",
        "def find_nearest_person(embedding, face_db, threshold):\n",
        "    min_distance = 999\n",
        "    person = \"Unknown\"\n",
        "    for embd, name in face_db:\n",
        "        distance, confidence = compare_faces(embd, embedding)\n",
        "        if distance < threshold and distance < min_distance:\n",
        "            min_distance = distance\n",
        "            person = name\n",
        "    return (person, min_distance)"
      ],
      "metadata": {
        "id": "IbsKuJSSiK0U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize MTCNN, load face embedding model and create known-faces database"
      ],
      "metadata": {
        "id": "SSDfLvp-nVUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### initialize mtcnn\n",
        "detector = MTCNN()\n",
        "mtcnn_min_conf = 0.9\n",
        "\n",
        "##### initialize face recognition model (saved_model)\n",
        "ckpt_dir = './ckpt/epoch_13'\n",
        "train_model = tf.keras.models.load_model(ckpt_dir)\n",
        "model = tf.keras.Model(train_model.get_layer('input').input, train_model.get_layer('embedding').output, trainable=False)\n",
        "#model.summary()\n",
        "\n",
        "# load known faces from folder\n",
        "known_faces_mtcnn_dir = './known_faces_mtcnn/'\n",
        "known_face_db = []\n",
        "for root, subdirectories, _ in os.walk(known_faces_mtcnn_dir):\n",
        "    for person in subdirectories:\n",
        "        print('processing person: ' + person)\n",
        "        for subroot, _, files in os.walk(os.path.join(root, person)):\n",
        "            for file in files:\n",
        "                full_path =  os.path.join(subroot, file)\n",
        "                print('processing file: ' + full_path)\n",
        "                img = cv2.imread(full_path)\n",
        "                img = cv2.resize(img, (112, 112))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = img - 127.5\n",
        "                img = img * 0.0078125\n",
        "                img_tensor = np.expand_dims(img, axis=0)\n",
        "                #print(img_tensor.shape)\n",
        "                result = model.predict(img_tensor, batch_size=1, verbose=0)\n",
        "                known_face_db.append((result.flatten(), person))\n",
        "                #known_face_db.append((result, person))\n",
        "\n",
        "#print(known_face_db)"
      ],
      "metadata": {
        "id": "NQhHjpZRiOES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process video file and run face recognition"
      ],
      "metadata": {
        "id": "VfVVgwDZnvn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read video and start processing loop\n",
        "# the video used in this repo comes from https://www.youtube.com/watch?v=R32qWdOWrTo\n",
        "# it can be downloaded using a Youtube downloader\n",
        "video_input_file = './nosedive.mp4'\n",
        "video_output_file = \"./output.mp4\"\n",
        "\n",
        "vidcap = cv2.VideoCapture(video_input_file)\n",
        "vidout = cv2.VideoWriter(video_output_file,cv2.VideoWriter_fourcc(*'MPEG'),30,(1920,1080))\n",
        "\n",
        "success,img = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "    print(\"processing frame: \" + str(count))\n",
        "\n",
        "    # BGR2RGB\n",
        "    img_with_dets = img.copy()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # MTCNN\n",
        "    detections = detector.detect_faces(img)\n",
        "\n",
        "    for det in detections:\n",
        "        if det['confidence'] >= mtcnn_min_conf:\n",
        "            x, y, width, height = det['box']\n",
        "            keypoints = det['keypoints']\n",
        "            cv2.rectangle(img_with_dets, (x,y), (x+width,y+height), (0,155,255), 2)\n",
        "            #cv2.circle(img_with_dets, (keypoints['left_eye']), 2, (0,155,255), 2)\n",
        "            #cv2.circle(img_with_dets, (keypoints['right_eye']), 2, (0,155,255), 2)\n",
        "            #cv2.circle(img_with_dets, (keypoints['nose']), 2, (0,155,255), 2)\n",
        "            #cv2.circle(img_with_dets, (keypoints['mouth_left']), 2, (0,155,255), 2)\n",
        "            #cv2.circle(img_with_dets, (keypoints['mouth_right']), 2, (0,155,255), 2)\n",
        "            \n",
        "            # pre-process the face image\n",
        "            bbox = det[\"box\"]\n",
        "            bbox = np.array([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])\n",
        "            landmarks = det[\"keypoints\"]\n",
        "\n",
        "            landmarks = np.array([landmarks[\"left_eye\"][0], landmarks[\"right_eye\"][0], landmarks[\"nose\"][0], landmarks[\"mouth_left\"][0], landmarks[\"mouth_right\"][0],\n",
        "                landmarks[\"left_eye\"][1], landmarks[\"right_eye\"][1], landmarks[\"nose\"][1], landmarks[\"mouth_left\"][1], landmarks[\"mouth_right\"][1]])\n",
        "            landmarks = landmarks.reshape((2,5)).T\n",
        "            nimg = face_preprocess.preprocess(img, bbox, landmarks, image_size='112,112')\n",
        "            \n",
        "            # calculate face embedding\n",
        "            nimg = nimg - 127.5\n",
        "            nimg = nimg * 0.0078125\n",
        "            nimg_tensor = np.expand_dims(nimg, axis=0)\n",
        "            result = model.predict(nimg_tensor, batch_size=1, verbose=0)\n",
        "            embedding = result.flatten()\n",
        "            \n",
        "            # find nearest face in db\n",
        "            dist_threshold = 1.0\n",
        "            name, dist = find_nearest_person(embedding, known_face_db, dist_threshold)\n",
        "            # TODO: set threshold for distance\n",
        "            if dist < dist_threshold:\n",
        "                img_with_dets = cv2.putText(img_with_dets, name, (x, y+height+50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255),2)\n",
        "                img_with_dets = cv2.putText(img_with_dets, str(dist), (x, y+height+100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255),2)\n",
        "\n",
        "\n",
        "    # write video frame\n",
        "    vidout.write(img_with_dets)\n",
        "    #cv2.imwrite(\"out_{}.jpg\".format(str(count)), img_with_dets)\n",
        "       \n",
        "    success,img = vidcap.read()\n",
        "    count += 1\n",
        "\n",
        "vidcap.release()\n",
        "vidout.release()\n",
        "\n",
        "print('Done processing frames from file!')\n",
        "\n",
        "# play a video file\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):\n",
        "   \n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        " \n",
        "show_video(video_output_file)"
      ],
      "metadata": {
        "id": "G66u_mIQjPlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}